// … oben unverändert …
import { scraperGuard } from '../services/scraper.guard.service';
import { SCRAPER_CONFIG } from '../config/scraper.config';
import { getRobots } from '../scrapers/base/RobotsCache';


// vor dem Start pro Job prüfen
if (await scraperGuard.disabled(seedbank)) {
logger.warn(`[Worker] ${seedbank} disabled by guard`);
return { created: 0, updated: 0, errors: 0, disabled: true } as any;
}


// robots.txt check auf Kategorie/URL‑Pfad
const conf = SCRAPER_CONFIG[seedbank as keyof typeof SCRAPER_CONFIG];
if (conf?.baseUrl) {
const allow = await getRobots(conf.baseUrl, conf.robotsTtlSec);
const path = new URL(url || conf.baseUrl).pathname || '/';
if (!allow(path)) {
logger.warn(`[Worker] robots.txt forbids ${seedbank} ${path}`);
await scraperGuard.record(seedbank, 'fail');
return { created: 0, updated: 0, errors: 1, robots: true } as any;
}
}


try {
// … Scraping wie bisher …
await scraperGuard.record(seedbank, 'ok');
} catch (e: any) {
const isBan = /403|captcha|forbidden|blocked/i.test(String(e?.message));
await scraperGuard.record(seedbank, isBan ? 'ban' : 'fail');
throw e;
}