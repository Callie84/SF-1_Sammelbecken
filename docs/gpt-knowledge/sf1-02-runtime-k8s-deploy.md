# SF-1 — Kubernetes Deploy (Stand: 2025-11-01)

## 1. Basis-Reihenfolge
1. k8s/secrets.yaml
2. k8s/mongo.yaml
3. k8s/backend.yaml
4. k8s/price-service.yaml
5. k8s/frontend.yaml
6. k8s/ingress-caddy.yaml

## 2. Erweiterungen (direkt im Root von k8s\)
- k8s/autoscaling.yaml
- k8s/cdn.yaml
- k8s/cert-issuer.yaml
- k8s/gateway.yaml
- k8s/search.yaml
- k8s/redis.yaml
- k8s/mongo-backup.yaml
- k8s/scraper-rqs.yaml
- k8s/scraper-zamnesia.yaml

## 3. Ordner-Struktur (Themen-Namespaces)
- k8s/analytics/* (namespace.yaml, plausible.yaml, clickhouse.yaml, ingress.yaml …)
- k8s/backup/* (00-namespace.yaml, 01-backup-pvc.yaml, CronJobs für mongodump/kubedump/rsync)
- k8s/monitoring/* (Prometheus, Loki, Grafana, Alertmanager, SLO-Rules)
- k8s/testing/* (00-namespace.yaml, smoke-CronJob)
- k8s/security/* (NetworkPolicies, Header, Labels)
- k8s/scraper/* (Config, Deployment-Worker, HPA, Service-Monitor)
- k8s/policies/* (supply-chain.yaml)
- k8s/restore-drill/* (cronjob.yml)

## 4. Vorgaben
- ingressClassName: caddy
- domain: seedfinderpro.de
- namespaces: default, monitoring, backup, testing
- Secrets nicht im Klartext committen ? k8s/secrets/*.yaml oder sealedsecret-*.yml

## 5. Apply-Hinweis
- Erst Root-Basis (Punkt 1)
- Dann thematische Ordner (analytics, monitoring, backup, security)
- Zuletzt ingress
# Kubernetes Deployment

Cluster: DebianÃ¢â‚¬â€˜basierte Nodes aufÃ‚Â Netcup, K8sÃ‚Â v1.30.  
Namespace: `default`

## Voraussetzungen
- `kubectl` + `helm` installiert
- `kubeconfig` liegt in `~/.kube/config`

## Deploy
# Server Setup (DebianÃ‚Â 12)

## SSH
# Lokales Setup

## Voraussetzungen
- Node.jsÃ‚Â Ã¢â€°Â¥Ã‚Â 20
- npmÃ‚Â Ã¢â€°Â¥Ã‚Â 10
- MongoDB lokal oder Atlas
- Git + PowerShell

## Installation
# SF-1 Autoscaling (HPA + VPA)

## Zweck
- **HPA** skaliert die Replikas der Deployments `sf1-backend` und `sf1-frontend` anhand von CPU- und RAM-Auslastung.
- **VPA** liefert **Empfehlungen** fÃ¼r Requests/Limits (updateMode=Off). Keine Live-Ã„nderungen.

## Dateien
- Manifest: `/k8s/autoscaling.yaml`
- ZugehÃ¶rige Deployments (vorausgesetzt): `apps/v1 Deployment sf1-backend`, `sf1-frontend` im Namespace `default`.

## Voraussetzungen
- metrics-server im Cluster aktiv (fÃ¼r CPU/RAM).
- Deployments besitzen **requests** und **limits**. Ohne `resources.requests` funktioniert **Resource-Utilization** im HPA nicht sinnvoll.
- VPA-Komponenten installiert (nur fÃ¼r Empfehlungen nÃ¶tig).

## Wirkprinzip (kurz)
- **Backend-HPA**: Ziel CPU 60 %, RAM 70 %, 2â€“8 Replikas. Ruhige Scale-Down-Phase (5 min Stabilisierung).
- **Frontend-HPA**: Ziel CPU 50 %, RAM 65 %, 2â€“6 Replikas.
- **PDBs** verhindern, dass alle Pods gleichzeitig evicted werden.
- **VPA** gibt Ober-/Untergrenzen vor (min/maxAllowed), Update **aus**.

## Risiken & Mitigation
- **Kein metrics-server** â†’ HPA bleibt statisch.  
  *Mitigation:* metrics-server bereitstellen.
- **Fehlende requests** â†’ HPA-Ziele sinnlos.  
  *Mitigation:* In Deployments `resources.requests` definieren (z. B. Backend: `cpu: 300m`, `memory: 600Mi`).
- **Zu aggressives Scaling** â†’ Flapping.  
  *Mitigation:* Bereits eingebaut: Stabilization und Policies; Schwellwerte nur vorsichtig Ã¤ndern.
- **VPA + HPA auf gleicher Ressource** â†’ Konflikt mÃ¶glich, wenn VPA live Ressourcen Ã¤ndert.  
  *Mitigation:* `updateMode=Off` belassen. Nur Empfehlungen nutzen und manuell ins Deployment Ã¼bernehmen.

## Validierung (nur Lesen, nichts ausfÃ¼hren)
- Manifeste prÃ¼fen: `kubectl kustomize` oder `kubectl apply --dry-run=client -f k8s/autoscaling.yaml`
- Metriken sichten (Grafana/Prometheus-Dashboards).
- VPA-Empfehlung ansehen (wenn VPA-CRDs vorhanden):  
  `kubectl get vpa -n default sf1-backend-vpa -o yaml` (zeigt Empfehlungen im Status-Bereich).

## Pflege
- Schwellenwerte halbjÃ¤hrlich prÃ¼fen (Traffic, Scraper-Last, Sale-Events).
- VPA-Empfehlungen regelmÃ¤ÃŸig in Deployments Ã¼bernehmen und committen.

**Stand:** 2025-10-17
# Deploy-Strategien: Blue/Green & Canary

## Ziel
Risikoarme Releases. Schnelles Rollback. Getrennte Canary-Tests ohne Einfluss auf den Hauptverkehr.

## Blue/Green (Hauptdomain)
- Zwei gleichwertige Stacks: **blue** und **green**.
- Der Service `sf1-backend`/`sf1-frontend` zeigt via Label-Selector auf **eine** Farbe.
- Umschalten = **nur** den `selector` des Service Ã¤ndern.

### Umschalten (Dokumentation, nichts ausfÃ¼hren)
- Backend live auf GREEN:
  - Service `sf1-backend.spec.selector` â†’ `{ app: sf1-backend, color: green }`
- Frontend live auf GREEN:
  - Service `sf1-frontend.spec.selector` â†’ `{ app: sf1-frontend, color: green }`

Rollback = zurÃ¼ck auf BLUE.

## Canary (Subdomain)
- Separate Subdomain `canary.seedfinderpro.de`.
- Eigene Deployments/Services mit Tag `:canary`.
- Nur Tester bekommen den Link. Kein Prozent-Splitting nÃ¶tig.

> Hinweis: Gewichtetem Routing (Prozent) wÃ¼rde ein Mesh (z. B. Istio/Linkerd) oder ein spezieller Ingress-Controller mit Traffic-Splitting bedÃ¼rfen. FÃ¼r KISS nutzen wir Subdomain-Canary.

## Risiken & Mitigation
- **Fehlerhafte Umschaltung** â†’ Downtime.
  - Mitigation: Services *nur* Ã¼ber Pull-Request Ã¤ndern, Smoke-Test vor Umschalten.
- **Unterschiedliche Configs** zwischen Blue/Green.
  - Mitigation: identische Manifeste; Farbe nur als Label.
- **TLS/Host fehlt** fÃ¼r Canary.
  - Mitigation: DNS-Eintrag `canary.seedfinderpro.de â†’ Ingress-IP` setzen. Cert-Manager stellt Zertifikat aus.

## Validierung (nur Lesekontrollen)
- Blue/Green bereit: `kubectl get deploy -l app=sf1-backend` und `...frontend`
- Aktive Farbe: `kubectl get svc sf1-backend -o jsonpath='{.spec.selector.color}'`
- Health: `https://seedfinderpro.de/api/health` und `https://canary.seedfinderpro.de/api/health`

**Stand:** 2025-10-17
# Troubleshooting

| Problem | Ursache | LÃƒÂ¶sung |
|----------|----------|---------|
| Kein Zugriff aufÃ‚Â Grafana | Ingress oder DNS falsch | `kubectl describe ingress grafana -n monitoring` |
| MongoÃ¢â‚¬â€˜Fehler Ã¢â‚¬Å¾ECONNREFUSEDÃ¢â‚¬Å“ | Service nicht gestartet | `kubectl get pods -n default`Ã‚Â Ã¢â€ â€™Ã‚Â Logs prÃƒÂ¼fen |
| Backup schlÃƒÂ¤gt fehl | S3Ã‚Â Secret fehlt | `kubectl get secrets -n backup` prÃƒÂ¼fen |
| CIÃ‚Â Ã¢â‚¬Å¾permission deniedÃ¢â‚¬Å“ | GHCRÃ‚Â Token unvollstÃƒÂ¤ndig | PATÃ‚Â mitÃ‚Â `write:packages` generieren |
| SmokeÃ‚Â CheckÃ‚Â fail | HealthÃ‚Â Endpoint falsch | `/api/health`Ã‚Â implementieren |
